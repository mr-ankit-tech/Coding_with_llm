{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiBw1nlg2rv/iAc2lGRjxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-ankit-tech/Coding_with_llm/blob/Workspace/Hugging_face_with_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=bFB4zqkcatU"
      ],
      "metadata": {
        "id": "NpzMP8Q49I5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fr_qTNdUb092"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "sec_key = userdata.get('HF_TOKEN')\n",
        "print(sec_key)"
      ],
      "metadata": {
        "id": "PsJ6grzMjwNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "QZTAkvbklTkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "sec_key = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "print(sec_key)"
      ],
      "metadata": {
        "id": "2gtA72p1mHw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=sec_key\n",
        "\n",
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm=HuggingFaceEndpoint(repo_id=repo_id,temperature=0.7)\n",
        "llm.invoke(\"What are the chapters in the book atomic habbits by james clear?\")\n"
      ],
      "metadata": {
        "id": "SBPEz6U3mKMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,LLMChain\n",
        "question = \"Who won the cricket world cup in the year 2011\"\n",
        "template =  \"\"\"Question :{question}\n",
        "Answer :\"\"\"\n",
        "prompt = PromptTemplate(template=template,input_variables=[\"question\"])\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "y63UCeywtLxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=llm,prompt=prompt)\n",
        "print(llm_chain.invoke(question))"
      ],
      "metadata": {
        "id": "9lcWzq8ZuEho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HuggingFacePipeline**"
      ],
      "metadata": {
        "id": "ZsUtFVsGvXbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline"
      ],
      "metadata": {
        "id": "WmPm48KnveyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "XfpPWZ8pxZAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)\n",
        "hf=HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "Hj3VkyXtyg57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke(\"Automation is real\")"
      ],
      "metadata": {
        "id": "UUNVkK1Ey5WK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}